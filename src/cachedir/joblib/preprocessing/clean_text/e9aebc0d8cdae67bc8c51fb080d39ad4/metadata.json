{"duration": 0.1714918613433838, "input_args": {"text": "'As the presidential inauguration drew near in January, something bordering on panic was taking hold among some scientists who rely on the vast oceans of data housed on government servers, which encompass information on everything from social demographics to satellite photographs of polar ice. In a Trump administration that has made clear its disdain for the copious evidence that human activity is warming the planet, researchers feared a broad crusade against the scientific information provided to the public. Reports last week that the administration is proposing deep budget cuts for government agencies including the National Oceanic and Atmospheric Administration and the Environmental Protection Agency have fueled new fears of databases being axed, if only as a   measure. \u201cWe\u2019ll probably be saying goodbye to much of the invaluable data housed at the NCEI,\u201d  Anne Jefferson, a water hydrology professor at Kent State University, wrote on Twitter Saturday, referring to the National Centers for Environmental Information. \u201cHope it gets rescued in time. \u201d It is illegal to destroy government data, but agencies can make it more difficult to find by revising websites and creating other barriers to the underlying information. Already there have been a handful of changes to the websites of federal science agencies, according to the Environmental Data and Governance Initiative, a new organization with researchers monitoring the content. On the E. P. A. \u2019s website, for instance, the science and technology office had described as its mission the development of \u201cscientific and technological foundations to achieve clean water. \u201d Now the office says the goal is to develop \u201ceconomically and technologically achievable performance standards. \u201d Pie charts on a Department of Energy website illustrating the link between coal and greenhouse gas emissions also have disappeared. So has the description on an Interior Department page of the potential environmental effects of hydraulic fracturing on federal land. Changes like these appear only to reflect the publicly stated priorities of the new administration and there have been few signs as yet that federal databases are being systematically manipulated or restricted. But concern about the vulnerability of scientific information has also focused attention on a nonpartisan problem of   government: Much of the scientific information so painstakingly collected over the decades, at a cost of hundreds of billions of dollars, remains held only by the government, scattered on thousands of servers in hundreds of departments where it may not be backed up and could be impossible to find. As thousands of academics, librarians, coders and   citizens have gathered at what are called \u201cdata rescue\u201d events in recent weeks  \u2014   there were at least six this past weekend alone  \u2014   the enormousness of extracting government data that is easily found has become apparent, as has the difficulty in tracking down the rest. Some   activists refer to it as \u201cdark data\u201d  \u2014   and they are not talking about classified information or data the government might release only if compelled by a Freedom of Information Act request. \u201cIt\u2019s like dark matter we know it must be there but we don\u2019t know where to find it to verify,\u201d said Maxwell Ogden, the director of Code for Science and Society, a nonprofit that began a   archiving project in collaboration with the research libraries in the University of California system. \u201cIf they\u2019re going to delete something, how will we even know it\u2019s deleted if we didn\u2019t know it was there?\u201d he asked. The obstacles have spurred debate among   activists over how to build an archiving system for the government\u2019s science data that ensures that the public does not lose access to it, regardless of who is in power. \u201cNo one would advocate for a system where the government stores all scientific data and we just trust them to give it to us,\u201d said Laurie Allen, a digital librarian at the University of Pennsylvania who helped found Data Refuge. \u201cWe didn\u2019t used to have that system, yet that is the system we have landed with. \u201d At the moment, the closest thing to a central repository is Data. gov, which, under a 2013 Obama administration directive, is supposed to link to all of the public databases within the government. But it relies on agencies to   and the total size of all the data linked to by the directory, Mr. Ogden recently found, comes to just 40 terabytes  \u2014   about as much as would fit on $1, 000 worth of hard drives. NASA alone provides access to more than 17. 5 petabytes of archived data, according to its website (a petabyte is 1, 000 times bigger than a terabyte) over dozens of different data portal systems. And   of the links on Data. gov, Mr. Ogden found, take users to a website rather than the actual data, which makes it hard to devise software that can automatically copy it. Even databases that are listed on Data. gov  \u2014   and there are more than two million, according to Mr. Ogden\u2019s published logs  \u2014   often sit behind an interface designed for ease of use but built with proprietary code almost impossible to reproduce. The need to write custom code to extract data from, say, the E. P. A. \u2019s discharge monitoring reports is one reason that, despite having hosted more than two dozen \u201cdata rescue\u201d events since January, the activist group Data Refuge lists only 158 data sets in its public directory. Andrew Bergman, a graduate student in applied physics at Harvard, along with two physics department colleagues, suspended his studies to help found the Environmental Data and Governance Initiative, which has also helped to organize the events. \u201cWe have things that are considered really important from NASA, E. P. A. NOAA,\u201d Mr. Bergman said. \u201cBut in terms of finalized, completed data sets that are actually useful, it\u2019s a very small number compared to the total. \u201d The transition to digital distribution that made government documents more accessible, librarians say, has also left them more at risk. Without physical copies in libraries, the internet\u2019s promise of making government information more widely available has made it far more centralized. Except when certain data is the subject of a lawsuit or multiple F. O. I. A. requests, it remains unclear what compels an agency to keep it online. \u201cDestroying federal records is a crime,\u201d said Patrice McDermott, who heads a public advocacy organization called Open the Government. \u201cTaking them off of the internet does not have the same penalty. \u201d In a recent letter to the federal Office of Management and Budget, Ms. McDermott\u2019s group cited a clause in the 1995 Paperwork Reduction Act that requires agencies to \u201cprovide adequate notice when initiating, substantially modifying, or terminating significant information dissemination products. \u201d But what that means for the age of big data has not been defined. To make secure copies of government research that researchers can trust is no easy task, librarians say. But many of those who have been trying for years to find funding and a system to do it reliably hope to harness the current wave of interest. \u201cAt the moment, more people than ever are aware of the risk of relying solely on the government to preserve its own information,\u2019\u2019 two government document librarians, James A. Jacobs, of the University of California, San Diego, and James R. Jacobs of Stanford University, wrote in an essay circulated online last week. \u201cThis was not true even six months ago. \u2019\u2019 At the archiving events, participants are typically divided into groups. One uses a web browser extension to flag government web addresses for the Internet Archive, an existing service that operates an automated \u201cweb crawler\u201d that can make copies of federal websites but typically not the databases that store information in more exotic formats. Another group is tasked with scrutinizing data sets that researchers have identified as particularly useful or vulnerable. Those are \u201ctagged\u201d with a description of where they came from and what they are. At one of last month\u2019s events, at New York University, many marveled at the breadth and depth of the research they were sorting through, even as they worried about its future. \u201cLook, you can get temperature and salinity readings from any one of these buoys,\u2019\u2019 said Barbara Thiers, the vice president for science at the New York Botanical Garden, another participant. \u201cThis is the raw data for tracking ocean warming. \u2019\u2019'"}, "time": 1742563789.5899856}